import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import os
from dotenv import load_dotenv
import random

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}
load_dotenv()

# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API ì„¤ì •
API_URL = "https://openapi.naver.com/v1/search/news.json"
HEADERS = {
    "X-Naver-Client-Id": os.getenv("NAVER_CLIENT_ID"),
    "X-Naver-Client-Secret": os.getenv("NAVER_CLIENT_SECRET")
}
# ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸
news_data = []

def scrape_news_content():
    for start in range(1, 51, 10):
        params = {
            "query": "ì •ì¹˜",
            "display": 10,
            "start": start,
            "sort": "date"
        }

        response = requests.get(API_URL, headers=HEADERS, params=params)

        # API ì‘ë‹µì´ ì •ìƒì¸ì§€ í™•ì¸
        if response.status_code == 200:
            news_list = response.json().get("items", [])

            for news in news_list:
                title = news["title"]  # ë‰´ìŠ¤ ì œëª©
                link = news["link"]  # ë‰´ìŠ¤ ë§í¬
                description = news['description']

                # ë„¤ì´ë²„ ë‰´ìŠ¤ë§Œ í¬ë¡¤ë§
                if link.startswith("https://n.news.naver.com"):
                    try:
                        # ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
                        news_response = requests.get(link, headers=headers, timeout=10, allow_redirects=False)
                        print(f"ğŸ”„ ì‘ë‹µ ì½”ë“œ: {response.status_code}")
                        if news_response.status_code == 200:
                            soup = BeautifulSoup(news_response.text, "html.parser")

                            # âœ… ì–¸ë¡ ì‚¬ ì´ë¦„ í¬ë¡¤ë§
                            press = soup.select_one("a.media_end_head_top_logo img")
                            press_name = press["alt"] if press else "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ"

                            # âœ… ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§
                            article = soup.select_one("article#dic_area")
                            article_text = article.get_text(strip=True, separator="\n") if article else "ë³¸ë¬¸ ì—†ìŒ"

                            MAX_LENGTH = 5000
                            if len(article_text) > MAX_LENGTH:
                                article_text = article_text[:MAX_LENGTH] + "..."  # 1000ìê¹Œì§€ë§Œ ì €ì¥

                            # ë°ì´í„° ì €ì¥
                            news_data.append({
                                "title": title,
                                "article": article_text,
                                "pressName": press_name,
                                "originallink": news['originallink'],
                                "link": news["link"],
                                "pubDate": news['pubDate'],
                                "description": description
                            })

                            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {title} ({press_name})")
                        else:
                            print("í¬ë¡¤ë§ ì‹¤íŒ¨!!")
                    except Exception as e:
                        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {link} - {e}")

                time.sleep(0.1)  # í¬ë¡¤ë§ ê°„ê²© ì¡°ì • (ì„œë²„ ì°¨ë‹¨ ë°©ì§€)
        else:
            print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨! ìƒíƒœ ì½”ë“œ: {response.status_code}")

    print(f"\nğŸ‰ ì´ {len(news_data)}ê°œì˜ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ!")

    csv_filename = "news_data_politic.csv"
    df = pd.DataFrame(news_data)
    df.to_csv(csv_filename, index=False, encoding="utf-8-sig")  # í•œê¸€ ê¹¨ì§ ë°©ì§€

    print(f"ğŸ“‚ ë°ì´í„°ê°€ '{csv_filename}' íŒŒì¼ë¡œ ì €ì¥ë¨!")

    return news_data
